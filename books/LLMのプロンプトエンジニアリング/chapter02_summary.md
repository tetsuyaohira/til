# LLMのプロンプトエンジニアリング 第2章まとめ

## 2章 LLMを理解する

### 2.1 LLMとは何か

LLMは本質的に「文字列を入力して文字列を返すサービス」である。入力を「プロンプト」、出力を「補完」または「レスポンス」と呼ぶ。トレーニングされていないLLMは、初めて動作するとき、補完としてランダムなUnicode記号の寄せ集めを生成し、プロンプトとは明確な関連性を持たない。役立つものにするには、「トレーニング」を施す必要がある。

LLMは「トレーニングデータセット」と呼ばれる大規模なドキュメントの集合を使ってトレーニングされる。トレーニングデータセットには、書籍、記事、Redditの会話、GitHub上のコードなど、さまざまな形式のデータが含まれることがある。モデルは、このデータセットから、トレーニングデータセットに似た出力を生成する方法を学習する。

重要な点は、LLMは単なる検索エンジンではないことである。検索エンジンであれば、ドキュメントの冒頭が与えられた場合、そのドキュメントの続き部分を100%正確に見つけ出すことができる。しかし、単にトレーニングデータを丸暗記し、それをなぞることは、LLMの目標ではない。LLMが目指しているのは、トレーニングデータをそっくり再現することではなく、その中で見出したパターン、特に論理的な推論や展開のしかたを抽象化し、トレーニングデータとは異なる新たなプロンプトに対しても、柔軟に補完できるようになることである。

#### ドキュメント補完
LLMは統計的に最も可能性の高い補完を選択する。この仕組みは、テキストを読むときに人間が行う前提とは異なる場合がある。なぜなら、人間がテキストを作るプロセスには、もっと広い文脈や意図が関与するからである。

トレーニングデータをよく知るほど、そのデータでトレーニングされたLLMの出力について直感を形成しやすくなる。あるプロンプトがどんな補完を生み出すか知りたい場合、「合理的な人間ならどう応答するか」を想像するのではなく、「このプロンプトで始まるドキュメントは、どのような流れで続く可能性があるか」を考えるべきである。

#### 人間の思考とLLMの処理の違い
LLMは、統計的に最もありそうな補完を選ぶ。この仕組みは、テキストを読むときに人間が行う前提とは異なる場合がある。人間がテキストを作るプロセスには、もっと広い文脈や意図が関与するが、LLMは検索など別のモードを持たない。そのため、事実確認や情報検索を行わずに推測に基づいて回答を生成することがある。

#### ハルシネーション
LLMが「トレーニングデータを模倣するマシン」としてトレーニングされているという事実には、残念な副作用がある。それが「ハルシネーション」（幻覚）である。これは、事実と異なるがもっともらしく見える情報を、モデルが自信を持って生成してしまう現象である。

ハルシネーションはモデルの視点では他の補完と区別がつかないため、「勝手に作り話をしないで」といったプロンプト指示はほとんど効果がない。その代わり、モデルに検証可能な背景情報を提供させるアプローチが一般的である。たとえば、モデルに出した結論の推論を説明させたり、独立して計算可能なデータや、参照リンク、検索可能なキーワードや詳細情報を含めるといった方法がある。

ハルシネーションは誘発されることもある。プロンプトが実在しないものについて言及すると、LLMは通常、それが本当に存在すると想定し続けてしまう。これが「真実バイアス」と呼ばれるものである。この真実バイアスを活用することもできるが、プログラムでプロンプトを生成する場合は要注意である。

### 2.2 LLMが見る世界

LLMは文字列をどのようにとらえているのか。私たちは文字列と聞くと文字の並びを思い浮かべるが、LLMが実際に見ているものはそれとは少し異なる。LLMも実際には文字列を一文字ずつ読んでいるわけではない。

人間と同様に、LLMも一文字ずつ読むわけではない。モデルにテキストを送信すると、それはまず「トークン」と呼ばれる複数文字のチャンクに分解される。通常、トークンは3～4文字長であるが、よく使われる単語や文字列にはより長いトークンも存在する。モデルが使用するトークンの集合を「語彙」と呼ぶ。

テキストを読むとき、モデルはまず「トークナイザー」にそれを通してトークン列に変換する。その後、そのトークン列が実際のLLMに送られる。そして、LLMは（内部的には数値で表現される）トークン列を生成し、それが再びテキストに変換されてユーザーに返される。

#### LLMと人間の認識の違い

**違いその1：LLMは決定論的なトークナイザーを使用する**
人間は、文字から単語への変換を曖昧に行っている。目にした文字列と似ている語を推測して認識するのである。一方でLLMは、決定論的なトークナイザーを使うので、綴り間違いが目立つ。たとえば「ghost」は1つのトークンだが、「gohst」という誤りは「g」「oh」「st」の3つに分解される。このようにまったく別物として扱われるため、LLMは誤字を簡単に見分けられる。

**違いその2：LLMは文字を1つずつゆっくり確認できない**
人間は、必要に応じてペースを落として一文字一文字を慎重に確認できるが、LLMは内蔵のトークナイザーを使うだけで、ゆっくり吟味することはできない。多くのLLMは、トレーニングセットを通じて各トークンを構成する文字を学習しているが、それでも、トークンを分解したり再組み立てしたりする必要がある文法的な処理は非常に難しくなる。

**違いその3：LLMは文字を人間とは異なる見方をする**
人間はトークンや文字に関して直感的な理解を持っている。私たちは文字を「目で見て」いるので、どの文字が丸みを帯びていて、どの文字が角張っているかを自然に感じ取れる。一方、モデルは何とか処理できたとしても、そのために多くの処理能力を割かなければならず、本来意図した応用的な処理に使える処理能力が減ってしまう。

#### トークン数を数える
トークナイザーとモデルは混在させて使えない。すべてのモデルは固定されたトークナイザーを使うので、自分が使うモデルのトークナイザーを理解しておく価値がある。

なぜなら、モデルにとってテキストの長さはトークン数で決まるからである。プロンプト内のトークンが増えれば、そのプロンプトを読み込む時間はほぼトークン数に比例して増える。出力を生成する時間も、出力されるトークン数に比例する。計算コストも同様で、1予測あたりに必要な計算能力はトークン数に比例する。

LLMは好きなだけ長いテキストを受け取り、好きなだけ長いテキストを出せるわけではない。モデルは「コンテキストウィンドウサイズ」より少ないトークン数のテキストを受け取り、補完結果もプロンプトと合わせてコンテキストウィンドウサイズを超えないように出力する。コンテキストウィンドウサイズは通常、数千トークン程度で、理論上はA4数ページから数十ページ、場合によっては数百ページ分にもなる。

### 2.3 1つのトークンずつ

実のところ、LLMはテキストからテキストへの変換を直接行っているわけではなく、トークンからトークンへの直接変換をしているわけでもない。実際には「複数のトークン」から「次の1つのトークン」を求め続けているのである。モデルはただ、必要なだけこの操作を繰り返し、単一トークンを積み重ねていくことで、最終的に整ったテキストを生み出している。

#### 自己回帰モデル
LLMに1回処理させると、統計的に最も可能性の高い「次のトークン」が得られる。その後、このトークンはプロンプトの末尾に付け足され、LLMは「新しいプロンプト」を前提として、再び統計的に最もあり得る次のトークンを求める。このように、1トークンずつ予測を行い、そのときの予測が前の予測に依存する仕組みは「自己回帰（オートリグレッシブ）」と呼ばれる。

このように、1ステップごとに1つずつトークンを生み出していく、ほとんど単調なパターンから、人間がテキストを打つ場合との大きな違いが見えてくる。人間は途中で立ち止まり、考え、振り返ることができるが、モデルは各ステップで必ずトークンを出し続けなければならない。モデルは「もう少し考える時間が欲しい」というような猶予は与えられないし、立ち止まることもできない。

一度トークンを出してしまうと、LLMはそのトークンを覆せない。後戻りして消去できないのである。また、以前の出力が誤りであることを後から指摘して取消そうともしない。

#### パターンと繰り返し
自己回帰の仕組みには、もうひとつ問題がある。それは、モデル自身が作ったパターンにはまり込んでしまうことである。LLMはパターン認識が得意なので、たまたま何らかのパターンが生じると、どこで打ち切るべきか見つけられなくなることがある。パターンができてしまうと、そのトークン時点でもパターンを壊すよりは続けた方が「それらしく」見えてしまうのである。その結果、極端に繰り返しの多い出力が発生してしまう。

### 2.4 temperatureと確率

LLMが「最も可能性の高いトークン」を求めることを学んだが、もう一層深く踏み込むと、モデルは実際には「あり得るすべてのトークンの確率」を計算してから、その中から1つを選び出していることがわかる。実際にトークンを選択する仕組みは「サンプリング」と呼ばれている。

LLMは最も可能性が高いトークンだけでなく、すべてのトークンに対する確率を計算している。多くのモデルは、これらの確率情報をユーザーに返すことができる。通常、モデルは「logprob」（トークン確率の自然対数）という形で返す。logprobが高いほど、モデルがそのトークンをより有力と考えていることを意味する。

常に「最も有力なトークン」だけが欲しいわけではないかもしれない。特に、生成したテキストを自動で検証して不適切なものを除外できるなら、いくつかの候補を出させて、その中から不適切なものを除外するという戦略も取れる。これを行う代表的な方法が、0より大きい「temperature」を用いることである。

temperatureは0以上の数値で、モデルにどれくらい「創造的」になってほしいかを示す：

- **temperature = 0**: 最も有力なトークンだけが欲しいとき（正確性が最優先、決定論的）
- **temperature = 0.1～0.4**: 最有力候補にごくわずかに劣る程度のトークンを、たまにでも拾ってほしい場合
- **temperature = 0.5～0.7**: より偶然性の影響を大きくしたい場合（多様性重視）
- **temperature = 1**: トレーニングデータ上の出現確率分布をそのまま反映させたい場合
- **temperature > 1**: トレーニングデータよりも「乱雑」な出力が欲しい場合

temperatureが1を超えると、LLMは酔っぱらったような出力をすることがある。1より大きいtemperatureで長いテキストを生成させると、時間経過とともに誤り率が徐々に増すことが多い。これは、temperatureが影響するのは確率を最終的に出力トークンへ変換する最後の層だけであり、もともとの確率計算を行うモデル本体の処理には作用しないからである。

### 2.5 トランスフォーマーアーキテクチャ

LLMの「頭脳」を直接見ていくと、そこには、実は1つの脳ではなく、無数の「小さな脳（ミニブレイン）」が存在している。それぞれ構造は同じで、よく似た役割を果たしている。シーケンス中の各トークンには1つずつミニブレインが割り当てられており、それらすべてをまとめたものが、あらゆる現代的LLMが採用するアーキテクチャ「トランスフォーマー」である。

各ミニブレインは、まず自分が担当するトークンと、そのトークンが文中のどの位置にあるかを知らされる。このミニブレインは「レイヤ（層）」と呼ばれる複数のステップを経る間、左側にいる別のミニブレインから情報を受け取りつつ、担当ポジションから見たドキュメントの内容を理解する。

すべてのミニブレインは、同じプロセス、つまり中間結果を計算し、それを共有して、最後に推測を行うという手順を踏む。実際、これらのミニブレインは互いにクローンのような存在で、処理の仕組みは共通している。違いがあるとすれば、最初に与えられるトークンや、左側から渡される中間結果といった、入力情報の部分だけである。

右端のトークンにあるミニブレインが予測を行うと、自己回帰が動き出す。新たなトークンが出力されると、そのうえにまた新たなミニブレインが配置され、あらかじめ定められた層数だけ処理を重ねて理解を深めた後、次のトークンを予測する。

#### アテンション機構
各ミニブレイン同士が共有する「中間結果」の共有方式は「アテンション機構」と呼ばれ、トランスフォーマーを支える中核的な発明である。アテンションは、ミニブレイン同士で情報を受け渡す手段で、数千ものミニブレインが互いに有用な情報を持つ可能性がある。

アテンション機構は質問・回答システムとして機能する：

1. 各ミニブレインは、「知りたいこと」をいくつか抱えており、それに答えられそうな他のミニブレインを探すための「質問」を発する
2. 各ミニブレインは、共有できるいくつかの情報を持っており、それが他のミニブレインに役立つ可能性を期待して、いくつかの情報を提示する
3. すると、各質問は最も適切な回答とマッチングされる
4. こうして選ばれた回答は、その質問を発したミニブレインに伝えられる

現代のLLMでは、この質疑応答の仕組みに、もう1つ「マスキング」という制約が加わっている。つまり、すべてのミニブレインが質問に答えられるわけではなく、質問しているミニブレインより「左側」にいるものだけが回答可能なのである。

#### 情報フローの制約
重要な制約として以下がある：

- **情報は常に左から右へしか流れない**
- **情報は常に下から上へしか流れない**

この三角形構造は、LLMが「後方かつ下方」に視線を固定されていることを示している。もう少しわかりやすく言えば、「後方かつ無知な方向」へしか目を向けられない、ということである。

**後方**: ミニブレインは常に左側を参照できるだけで、どれほど後方を見ることはできても、前方を見ることはできない。これが、GPTなどのLLMが「一方向」トランスフォーマーと呼ばれる理由である。

**下方（「無知な方向」）**: 同じレイヤにいるミニブレインは、そのレイヤで自分が答えを得る前に計算を終えた左側のミニブレインからしか情報を受け取れない。また、後のレイヤで得られた洞察を、前のレイヤ（より下層）へ戻して再処理することは不可能である。

#### 並列処理と逐次処理
並列性によって高速化は可能だが、モデルがプロンプトの読み取りから補完テキストの生成へ移行する段階で、この「三角形的」な計算方式は崩れる。モデルは、あるトークンの処理が完全に終わるまで待たないと、次のトークンを選んで新しいミニブレインの初期状態を計算できない。このため、LLMは長いプロンプトを読み込むのは得意だが、長い補完テキストを生成するのはずっと遅くなる。

#### LLMの根本的制約
これが、プロンプトエンジニアリングにおいて順序がいかに重要であるかの理由である。順序が違うだけで、うまくいくプロンプトと失敗するプロンプトに分かれてしまう。

例えば、文字数を数える問題において、人間なら「何文字？」と聞かれてから文章を読み直して数えることができるが、LLMは文章を一度しか読まないため、後から「文字数を数えろ」というリクエストが出てくることを知らない。ミニブレインたちが段落を処理している段階では、その後の質問を知らないのである。

## まとめ

第2章では、LLMの内部動作について以下の重要な事実を学んだ：

1. **LLMはドキュメント補完エンジンである**
2. **トレーニングで見たドキュメントを模倣する**
3. **1トークンずつしか生成せず、途中で修正や戻り作業ができない**
4. **テキストを最初から最後まで一度しか読まない**

これらの制約は、LLMが真の意味での「知能」なのか、それとも高度な統計的パターンマッチングなのかという根本的な問題を提起している。特に「後戻りできない」「一度しか読めない」という制約は、人間の知能が持つ柔軟性や修正能力との重要な相違点として認識される。

トランスフォーマーアーキテクチャの理解により、LLMは無数のミニブレインが協力して文脈理解を行う高度なシステムであることがわかった。しかし、その制約もまた、現在のLLMの限界を示している。これらの理解は、効果的なプロンプトエンジニアリングの基礎となる。